{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df = pd.read_csv('./data/3775610.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resturture the data files (Pandas)\n",
    "\n",
    "def restructure_csv_files(directory, ref_df, new_directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return\n",
    "\n",
    "    ref_columns = ref_df.columns.tolist()\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(directory, file))\n",
    "\n",
    "        if df.columns.tolist() != ref_columns:\n",
    "            # Add missing columns\n",
    "            df = df.loc[:, [i for i in ref_columns if i in df.columns]]\n",
    "            df.loc[:, [i for i in ref_columns if i not in df.columns]] = None\n",
    "\n",
    "            # Reorder columns and save the DataFrame\n",
    "            df = df.loc[:, ref_columns]\n",
    "            df.to_csv(os.path.join(new_directory, file), header=True, index=False, sep=',')\n",
    "\n",
    "# Call the function with your directory and reference DataFrame\n",
    "restructure_csv_files('./data/', ref_df, './processed data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def check_csv_structure(directory):\\n    files = [f for f in os.listdir(directory) if f.endswith(\\'.csv\\')]\\n    if not files:\\n        print(\"No CSV files found in the directory.\")\\n        return []\\n\\n    # Read the first file to get the reference structure\\n    # ref_df = pd.read_csv(os.path.join(directory, files[0]))\\n    ref_columns = ref_df.columns.tolist()\\n\\n    different_structure_files = []\\n    for file in files[1:]:\\n        df = pd.read_csv(os.path.join(directory, file), sep=\\',\\')\\n        if df.columns.tolist() != ref_columns:\\n            different_structure_files.append(file)\\n\\n    return different_structure_files\\n\\n# Call the function with your directory\\ndifferent_files = check_csv_structure(\\'./processed data/\\')\\nprint(different_files)'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def check_csv_structure(directory):\n",
    "    files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    if not files:\n",
    "        print(\"No CSV files found in the directory.\")\n",
    "        return []\n",
    "\n",
    "    # Read the first file to get the reference structure\n",
    "    # ref_df = pd.read_csv(os.path.join(directory, files[0]))\n",
    "    ref_columns = ref_df.columns.tolist()\n",
    "\n",
    "    different_structure_files = []\n",
    "    for file in files[1:]:\n",
    "        df = pd.read_csv(os.path.join(directory, file), sep=',')\n",
    "        if df.columns.tolist() != ref_columns:\n",
    "            different_structure_files.append(file)\n",
    "\n",
    "    return different_structure_files\n",
    "\n",
    "# Call the function with your directory\n",
    "different_files = check_csv_structure('./processed data/')\n",
    "print(different_files)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "import ast\n",
    "import os\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--driver-class-path /home/lymf/mysql-connector-j-8.1.0.jar pyspark-shell'\n",
    "\n",
    "# Create PySpark SparkSession\n",
    "spark = SparkSession.builder.appName(\"pandas to spark\").getOrCreate()\n",
    "\n",
    "# Define a UDF to convert strings to lists using ast.literal_eval\n",
    "# def literal_eval_udf(x):\n",
    "#     return ast.literal_eval(x) if x is not None else x\n",
    "\n",
    "# literal_eval_udf = udf(literal_eval_udf, ArrayType(StringType()))\n",
    "\n",
    "# Define a UDF to extract the last name from the 'player' column\n",
    "def get_last_name(x):\n",
    "    return x.split(' ')[-1] if x is not None else x\n",
    "\n",
    "get_last_name_udf = udf(get_last_name, StringType())\n",
    "\n",
    "# Read all CSV files in a folder into a DataFrame\n",
    "df = spark.read.format('csv').option('header', 'true').option('quote', '\\\"').option('escape', '\\\"').load(\"./processed data/*.csv\")\n",
    "\n",
    "# df = df.withColumn('player_last_name', get_last_name_udf(df['player']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Database connection URL\n",
    "url = \"jdbc:mysql://localhost:3306/ftdata\"\n",
    "\n",
    "# Properties\n",
    "properties = {\n",
    "    \"user\": \"debian-sys-maint\",\n",
    "    \"password\": \"vMCs6xaR5jYNdv3u\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "df.write.jdbc(url=url, table=\"match_events\", mode=\"overwrite\", properties=properties)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
