{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import models\n",
    "import eventstox\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [6, 4]\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1819 = pd.read_csv('df_1819.csv')\n",
    "df_1920 = pd.read_csv('df_1920.csv')\n",
    "df_2021 = pd.read_csv('df_2021.csv')\n",
    "\n",
    "X_1819, y_1819 = eventstox.df_to_X_y(df_1819)\n",
    "X_1920, y_1920 = eventstox.df_to_X_y(df_1920)\n",
    "X_2021, y_2021 = eventstox.df_to_X_y(df_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = pd.concat([X_1819, X_1920], axis=0).reset_index(drop=True)\n",
    "y = np.concatenate([y_1819, y_1920], axis=0)\n",
    "\n",
    "# drop unecessary columns\n",
    "X = X.drop(columns=[col for col in X.columns if 'type' in col])\n",
    "X = X.drop(columns=['location_x_10', 'location_y_10', 'shot_angle'])\n",
    "\n",
    "# get binary features\n",
    "X_binary = X[[col for col in X.columns if (\n",
    "    ('team' in col) | ('outcome' in col))]]\n",
    "\n",
    "# standard scaling on numerical features (locations)\n",
    "X_numerical = X.drop(\n",
    "    columns=X_binary.columns)\n",
    "\n",
    "scaler = MinMaxScaler((0, 1))\n",
    "X_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_numerical), \n",
    "    columns=X_numerical.columns,\n",
    ")\n",
    "\n",
    "X_scaled = pd.concat([X_binary, X_scaled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lstm_X(X_scaled):\n",
    "    X_arr = np.zeros((X_scaled.shape[0], 10, 6))\n",
    "\n",
    "    # List of features\n",
    "    features = [\"team\", \"outcome\", \"location_x\",\n",
    "            \"location_y\", \"end_location_x\", \"end_location_y\"]\n",
    "\n",
    "    # Iterate over each feature and timestamp to fill the array\n",
    "    for i, feature in enumerate(features):\n",
    "        for timestamp in range(10):\n",
    "            column_name = f\"{feature}_{timestamp}\"\n",
    "            X_arr[:, timestamp, i] = X_scaled[column_name]\n",
    "\n",
    "    return X_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Loss: 0.22705406248569487\n",
      "Average Validation Loss: 0.6288111865520477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Function to create a new LSTM model\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    # Single LSTM layer\n",
    "    model.add(Bidirectional(LSTM(50, input_shape=(10, 6))))\n",
    "    # Output layer with sigmoid activation for binary classification\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_evaluate_model(X_train, y_train, X_val, y_val):\n",
    "    # Suppress TensorFlow logging\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "    # Oversample the training set\n",
    "    oversampler = RandomOverSampler(random_state=0)\n",
    "    X_train_resampled, y_train_resampled = oversampler.fit_resample(\n",
    "        X_train, y_train)\n",
    "\n",
    "    # Convert to shape (10, 6)\n",
    "    X_train_resampled = get_lstm_X(X_train_resampled)\n",
    "    X_val = get_lstm_X(X_val)\n",
    "\n",
    "    # Train the model\n",
    "    model = create_model()\n",
    "    history = model.fit(X_train_resampled, y_train_resampled,\n",
    "                        epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "    # Get final training loss\n",
    "    final_training_loss = history.history['loss'][-1]\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    validation_loss, _ = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "    return final_training_loss, validation_loss\n",
    "\n",
    "\n",
    "# 5-Fold Cross-validation\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "# Using joblib to parallelize the cross-validation\n",
    "results = Parallel(n_jobs=-1)(delayed(train_evaluate_model)(\n",
    "    X_scaled.iloc[train_index], y[train_index], X_scaled.iloc[val_index], y[val_index]\n",
    ") for train_index, val_index in kf.split(X_scaled)\n",
    ")\n",
    "\n",
    "# Unpack results\n",
    "training_losses, validation_losses = zip(*results)\n",
    "\n",
    "print(\"Average Training Loss:\", np.mean(training_losses))\n",
    "print(\"Average Validation Loss:\", np.mean(validation_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggested hyperparameters\n",
    "    lstm_units = trial.suggest_categorical('lstm_units', [20, 50, 100])\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
    "\n",
    "    # Modify the create_model function to accept hyperparameters\n",
    "    def create_model(lstm_units, dropout_rate, learning_rate, optimizer):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(lstm_units, return_sequences=True, input_shape=(10, 6)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(lstm_units))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        if optimizer == 'adam':\n",
    "            opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        else:\n",
    "            opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    for train_index, val_index in kf.split(X_scaled):\n",
    "        X_train, X_val = X_scaled.iloc[train_index], X_scaled.iloc[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        X_train, y_train = RandomOverSampler().fit_resample(X_train, y_train)\n",
    "        X_train = get_lstm_X(X_train)\n",
    "        X_val = get_lstm_X(X_val)\n",
    "\n",
    "        model = create_model(lstm_units, dropout_rate,\n",
    "                             learning_rate, optimizer)\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "        loss, accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss)\n",
    "\n",
    "    average_accuracy = np.mean(accuracies)\n",
    "    average_loss = np.mean(losses)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "# Run the Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Best hyperparameters\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
